{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Association Rules Mining in Python\n",
    "\n",
    "\n",
    "Welcome to this tutorial on Market Basket Analysis, focusing on the basics of implementing the Apriori Algorithm and Association Rule Mining in Python.\n",
    "We'll explore how to use Python to perform Market Basket Analysis, a popular application of Association Rules Mining, which is a powerful technique used to uncover interesting relationships and patterns in transactional data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "These materials have been adapted from: <br/>\n",
    "\n",
    "- [Frequent itemsets via the Apriori algorithm](https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)\n",
    "\n",
    "- [Association rules generation from frequent itemsets](https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Market Basket Analysis?\n",
    "\n",
    "Market Basket Analysis (MBA) is a data mining technique used to identify the relationships between items purchased together by customers. It originated from the retail industry, particularly from the concept of analyzing transactions in a physical or online store. The name \"market basket\" comes from the analogy of customers' shopping baskets filled with items they intend to purchase.\n",
    "\n",
    "\n",
    "- **Market basket analysis**\n",
    "    - Construct association rules\n",
    "        - Association rules are generated to identify patterns in the dataset.\n",
    "        - These rules reveal which items are frequently purchased together, providing insights into customer behavior.\n",
    "\n",
    "- **Association rules**\n",
    "    - Association rules are logical expressions that capture relationships between sets of items in a transaction.\n",
    "        - **{antecedent}→{consequent}**\n",
    "            - An association rule is typically represented as {antecedent}→{consequent}\n",
    "            - For example, if customers frequently purchase fiction books ({antecedent}), they are also likely to buy biography books ({consequent}). {fiction}→{biography}\n",
    "\n",
    "\n",
    "### Some cases of market basket analysis\n",
    "\n",
    "1. **Improve product recommendations on an e-commerce store.**\n",
    "    \n",
    "    MBA helps e-commerce platforms suggest related or complementary products to customers based on their purchase history.\n",
    "\n",
    "2. **Developing Netflix-style Recommendations Engine:**\n",
    "\n",
    "    Media streaming platforms like Netflix utilize MBA to recommend movies or TV shows to users based on their viewing history, preferences, and similar viewing patterns of other users.\n",
    "\n",
    "3. **Optimizing Inventory Management:**\n",
    "\n",
    "    MBA assists in optimizing inventory levels by identifying which products are often purchased together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Apriori Algorithm\n",
    "\n",
    "The **Apriori algorithm** is a widely recognized machine learning technique employed for association rule learning. Association rule learning involves analyzing a dataset to identify relationships between items within the data. For instance, in a dataset containing grocery store items, association rule learning can help uncover items frequently bought together. \n",
    "\n",
    "\n",
    "\n",
    "### Metrics and Pruning\n",
    "\n",
    "\n",
    "In order to select the most relevant rules from the multitude of possibilities in a business scenario, we rely on metrics:\n",
    "\n",
    "- A metric serves as a measure of performance for rules, providing insights into their significance.\n",
    "  - Example:\n",
    "    - {humor} → {poetry}: 0.81 &#x2713;\n",
    "    - {fiction} → {travel}: 0.23 &#x2717;\n",
    "- Pruning involves using metrics to discard irrelevant or uninformative rules, ensuring that only meaningful rules are retained.\n",
    "  - Example:\n",
    "    - Retain: {humor} → {poetry} &#x2713;\n",
    "    - Discard: {fiction} → {travel}\n",
    "\n",
    "#### The Support Metric (the Simplest Metric):\n",
    "\n",
    "- The support metric measures the proportion of transactions that contain a specific itemset.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{support} = \\frac{\\text{number of transactions with item(s)}}{\\text{total number of transactions}}\n",
    "  $$\n",
    "\n",
    "#### The Confidence Metric:\n",
    "\n",
    "- Confidence complements support by providing a more comprehensive understanding of the relationship between items.\n",
    "- It indicates the probability of purchasing item $Y$ given that item $X$ has been purchased.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{confidence}(X \\rightarrow Y) = \\frac{\\text{support}(X \\cap Y)}{\\text{support}(Y)}\n",
    "  $$\n",
    "\n",
    "#### The Lift Metric:\n",
    "\n",
    "- Lift offers another perspective on item relationships, considering the proportion of transactions containing both items relative to random and independent assignment.\n",
    "- Lift values greater than 1 suggest a significant association between items.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{lift}(X \\rightarrow Y) = \\frac{\\text{support}(X \\cap Y)}{\\text{support}(X) \\times \\text{support}(Y)}\n",
    "  $$\n",
    "\n",
    "#### The Conviction Metric:\n",
    "\n",
    "- Conviction evaluates the impact of the absence of the antecedent on the consequent, indicating the degree to which the consequent would be hurt.\n",
    "- Higher conviction values signify a stronger interest in the rule.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{conviction}(X \\rightarrow Y) = \\frac{1 - \\text{support}(Y)}{1 - \\text{confidence}(X \\rightarrow Y)}\n",
    "  $$\n",
    "\n",
    "#### The Leverage Metric:\n",
    "\n",
    "- Leverage, akin to lift, measures the difference between the observed and expected frequency of co-occurrence.\n",
    "- It provides a more interpretable metric within the range of [-1, 1].\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{leverage}(X \\rightarrow Y) = \\text{support}(X \\cap Y) - \\text{support}(X) \\times \\text{support}(Y)\n",
    "  $$\n",
    "\n",
    "\n",
    "### Algorithm steps\n",
    "\n",
    "### Apriori Algorithm: Explained and Expanded\n",
    "\n",
    "The Apriori algorithm is a fundamental technique used in Market Basket Analysis to discover association rules among items in transactional data. Let's delve into each step of the algorithm and illustrate with examples:\n",
    "\n",
    "#### Step 1: Generate Initial Itemsets\n",
    "\n",
    "- **Description:** Start with itemsets containing just a single item (individual items).\n",
    "- **Example:**\n",
    "\n",
    "| Itemset |\n",
    "|---------|\n",
    "| {Milk}  |\n",
    "| {Bread} |\n",
    "| {Eggs}  |\n",
    "| {Cheese}|\n",
    "\n",
    "#### Step 2: Determine Support for Itemsets\n",
    "\n",
    "- **Description:** Calculate the support for each itemset, indicating the frequency of occurrence in the dataset.\n",
    "- **Example:**\n",
    "\n",
    "| Itemset | Support |\n",
    "|---------|---------|\n",
    "| {Milk}  | 0.7     |\n",
    "| {Bread} | 0.6     |\n",
    "| {Eggs}  | 0.5     |\n",
    "| {Cheese}| 0.2     |\n",
    "\n",
    "#### Step 3: Prune Itemsets\n",
    "\n",
    "- **Description:** Keep the itemsets that meet the minimum support threshold (0.4) and discard itemsets that do not meet the minimum support.\n",
    "- **Example:**\n",
    "\n",
    "| Itemset | Support | Pruned? |\n",
    "|---------|---------|---------|\n",
    "| {Milk}  | 0.7     | ✓       |\n",
    "| {Bread} | 0.6     | ✓       |\n",
    "| {Eggs}  | 0.5     | ✓       |\n",
    "| {Cheese}| 0.2     |         |\n",
    "\n",
    "#### Step 4: Generate Candidate Itemsets\n",
    "\n",
    "- **Description:** Using the itemsets kept from Step 3, generate all possible combinations of itemsets.\n",
    "- **Example:**\n",
    "\n",
    "| Candidate Itemset | Support | Pruned? |\n",
    "|-------------------|---------|---------|\n",
    "| {Milk, Bread}     | 0.5     | ✓       |\n",
    "| {Milk, Eggs}      | 0.4     | ✓       |\n",
    "| {Bread, Eggs}     | 0.3     |         |\n",
    "\n",
    "\n",
    "#### Step 5: Repeat Until Convergence\n",
    "\n",
    "- **Description:** Repeat steps 1 to 4 until there are no more new itemsets generated.\n",
    "- **Example:** Continue iterating until convergence is reached, and no new itemsets are generated.\n",
    "\n",
    "\n",
    "#### Final Result: \n",
    "\n",
    "- After several iterations, the algorithm converges to the following frequent itemsets:\n",
    "\n",
    "| Itemset       | Support |\n",
    "|---------------|---------|\n",
    "| {Milk}        | 0.7     |\n",
    "| {Bread}       | 0.6     |\n",
    "| {Eggs}        | 0.5     |\n",
    "| {Milk, Bread} | 0.5     |\n",
    "| {Milk, Eggs}  | 0.4     |\n",
    "\n",
    "By following these steps, the Apriori algorithm efficiently identifies frequent itemsets and generates association rules from transactional data, providing valuable insights into customer purchasing patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Apriori algorithm in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
